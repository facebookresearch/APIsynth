{
  "metadata": {
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "bento_kernel_pytorch",
      "metadata": {
        "kernel_name": "bento_kernel_pytorch",
        "nightly_builds": true,
        "fbpkg_supported": true,
        "is_prebuilt": true
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "captumWidgetMessage": {},
    "last_msg_id": "038db179-4ab37f45f011cba41bb10d0e_641",
    "last_server_session_id": "6a880cd5-b005-488e-96e7-14a9715fae2f",
    "last_kernel_id": "60d479f4-201c-4ed6-a8ec-3dd1cc89b718",
    "last_base_url": "https://devvm3630.vll0.facebook.com:8090/",
    "outputWidgetContext": {}
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "originalKey": "32d111ed-dc6b-4407-aa14-fe8f7634343c",
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "32d111ed-dc6b-4407-aa14-fe8f7634343c",
        "executionStartTime": 1632950575800,
        "executionStopTime": 1632950576181
      },
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch as T\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "# device = T.device(\"cuda\") \n",
        "device = T.device(\"cpu\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "bf5b027c-8ae3-41a6-9351-1f4bbaeecee2",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "bf5b027c-8ae3-41a6-9351-1f4bbaeecee2",
        "code_folding": [],
        "hidden_ranges": [],
        "executionStartTime": 1632950577004,
        "executionStopTime": 1632950577332
      },
      "source": [
        "class pycoder_parameters:\n",
        "\n",
        "    ''' Path '''\n",
        "    # Path =  'exhaustive_17api' #'seq_30api'\n",
        "    Path = 'gen1_33'\n",
        "    # file_name = '3seq_synthetic'\n",
        "\n",
        "    ''' Core Fuzzing Parameters '''\n",
        "    NUM_FUZZ_PER_API= 100001 #000\n",
        "    NUM_TEST_FUZZ = 2\n",
        "    FLOAT_TENSOR = False #We either generate float or integer tensors\n",
        "    UNIT_TEST = False\n",
        "    COMPOSITE = True\n",
        "\n",
        "    ''' Fuzzing Detailed Parameters '''\n",
        "    MAX_TENSOR_DIMENSIONS = 3 #how many rows, columns, etc.\n",
        "    MIN_VAL_PER_DIMENSION = 1 # e.g., min number of rows, columns, etc. \n",
        "    MAX_VAL_PER_DIMENSION = 5 # e.g., max number of rows, columns, etc. \n",
        "\n",
        "    #So far limiting to integers\n",
        "    MIN_TENSOR_VALUE = 1\n",
        "    MAX_TENSOR_VALUE = 15\n",
        "    \n",
        "\n",
        "    ''' Embedding Parameters '''\n",
        "    EMBEDDING_NOISE_LEVEL = 0 #0 noise by default\n",
        "    EMBEDDING_SIZE = 150\n",
        "    SHAPE_EMBEDDING_SIZE = 6\n",
        "\n",
        "\n",
        "    data_type = 'float' if FLOAT_TENSOR is  True else 'integer'\n",
        "    model_type = 'Composite_' if COMPOSITE is  True else 'Single_'\n",
        "    file_name = str(model_type) + str(NUM_FUZZ_PER_API) + '_' + data_type\n",
        "    fuzzing   = file_name + '.pt'\n",
        "    embedding = file_name + '.embedding' + '.pt',\n",
        "    classification = file_name + '.model_result' + '.pt' \n",
        "    train_valid_test = file_name + 'train_valid_test.pt'\n",
        "\n",
        "    def setNoiseLevel(self, noise):\n",
        "        self.EMBEDDING_NOISE_LEVEL = noise\n",
        "        self.embedding = self.file_name + '.embedding' + '_' + str(self.EMBEDDING_NOISE_LEVEL) + '.pt'\n",
        "\n",
        "    def getEmbeddingFile(self):\n",
        "        return(self.file_name + '.embedding' + '_' + str(self.EMBEDDING_NOISE_LEVEL) + '.pt')\n",
        "\n",
        "    def getVisulizationFile(self):\n",
        "        return(self.file_name + '.embedding' + '_' + str(self.EMBEDDING_NOISE_LEVEL) + '_' +  'tSNE.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "34c2a89d-edfb-4fcb-8c37-dba498fbe66d",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "34c2a89d-edfb-4fcb-8c37-dba498fbe66d",
        "code_folding": [],
        "hidden_ranges": [],
        "executionStartTime": 1632950577707,
        "executionStopTime": 1632950578031
      },
      "source": [
        "NOISE = 0\n",
        "f = pycoder_parameters()\n",
        "f.setNoiseLevel(NOISE)\n",
        "f.embedding = f.getEmbeddingFile() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "d601d98d-3130-4a68-8f8b-d5ccd0edb98e",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "d601d98d-3130-4a68-8f8b-d5ccd0edb98e",
        "code_folding": [],
        "hidden_ranges": [],
        "executionStartTime": 1632950579060,
        "executionStopTime": 1632950579399
      },
      "source": [
        "def add_noise(orig_tensor):\n",
        "    \n",
        "    orig_tensor = orig_tensor.double()\n",
        "    data = torch.flatten(orig_tensor).numpy()    \n",
        "    zero_like = torch.flatten(torch.zeros_like(orig_tensor,dtype=bool))\n",
        "    mask = zero_like.numpy()\n",
        "\n",
        "    elem_size = np.prod(list(orig_tensor.shape))\n",
        "    N =  int(elem_size * f.EMBEDDING_NOISE_LEVEL)\n",
        "    mask[:N] = True\n",
        "    \n",
        "    np.random.shuffle(mask)\n",
        "    data[mask] = 0\n",
        "    return(torch.Tensor(data))\n",
        "\n",
        "t1 = torch.tensor([1,2,3,4,5]) #[[1,2,3]],[[4,5,6]], [[7,8,9]]])\n",
        "print(t1, add_noise(t1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "368ca20a-d0af-4682-829b-3f26c5f8c40a",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "368ca20a-d0af-4682-829b-3f26c5f8c40a",
        "executionStartTime": 1632950579550,
        "executionStopTime": 1632950579834
      },
      "source": [
        "EMBEDDING_SIZE = f.EMBEDDING_SIZE\n",
        "SHAPE_EMBEDDING_SIZE = f.SHAPE_EMBEDDING_SIZE\n",
        "\n",
        "def encode_values_to_code(tensor):\n",
        "    tensor = tensor.clone()\n",
        "    tensor[(tensor>=100) & (tensor<1000)] = 100\n",
        "    tensor[(tensor>=1000)] = 101\n",
        "    tensor[(tensor<=-20) & (tensor>-100)] = -20\n",
        "    tensor[(tensor<=-100) & (tensor>-1000)] = -21\n",
        "    tensor[(tensor<=-1000)] = -22\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def tensor_flatten_pad(tensor, embed_size=EMBEDDING_SIZE, shape_embed_size=SHAPE_EMBEDDING_SIZE, isNoise=False):\n",
        "    \n",
        "    t_flatten = torch.flatten(tensor)\n",
        "\n",
        "    if isNoise is True:\n",
        "        t_flatten = add_noise(t_flatten)\n",
        "    padding_length = embed_size - list(t_flatten.shape)[-1]\n",
        "    p1d = (0,padding_length) #just padding the last dimension\n",
        "    t_pad = F.pad(input=t_flatten, pad=p1d, mode='constant', value=0).type(torch.FloatTensor)\n",
        "    \n",
        "    type_padding = 0\n",
        "    if tensor.dtype == torch.bool:\n",
        "        type_padding = 1\n",
        "    elif tensor.dtype == torch.float64 \\\n",
        "        or tensor.dtype == torch.double \\\n",
        "        or tensor.dtype == torch.float32 \\\n",
        "        or tensor.dtype == torch.float16:\n",
        "            type_padding = 2\n",
        "    \n",
        "    \n",
        "    '''size embedding'''\n",
        "    if(shape_embed_size > 0):\n",
        "        t_shape = list(tensor.shape)\n",
        "        padding_length = shape_embed_size -1 - len(t_shape)\n",
        "        p1d = (0,padding_length) #just padding the last dimension\n",
        "        s_pad = F.pad(input=torch.Tensor(t_shape), pad=p1d, mode='constant', value=0).type(torch.float)\n",
        "\n",
        "        t_pad_list = t_pad.tolist()\n",
        "        s_pad_list = s_pad.tolist()\n",
        "        tensor_embedding = torch.Tensor([type_padding] + [-1] + t_pad_list + [-1] + s_pad_list + [-1])\n",
        "    \n",
        "    else:\n",
        "        t_pad_list = t_pad.tolist()\n",
        "        tensor_embedding = torch.Tensor([type_padding] + [-1] + t_pad_list + [-1])\n",
        "        \n",
        "    encoded_tensor = encode_values_to_code(tensor_embedding)\n",
        "    return(encoded_tensor)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "daa7a542-224e-4365-9ff6-98cf30b948e6",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "daa7a542-224e-4365-9ff6-98cf30b948e6",
        "executionStartTime": 1632950580581,
        "executionStopTime": 1632950580905
      },
      "source": [
        "def sample_dataset(orig_dataset, frac=0.9):\n",
        "\n",
        "    print('len orig_dataset', len(orig_dataset))\n",
        "  \n",
        "    dataset =  orig_dataset #sample(orig_dataset,len(orig_dataset)//10)\n",
        "\n",
        "    print('len dataset', len(dataset))\n",
        "\n",
        "    length = len(dataset)\n",
        "    frac_length = int(length * frac)\n",
        "\n",
        "    idx = list(range(length))  # indices to all elements\n",
        "    random.shuffle(idx)  # in-place shuffle the indices to facilitate random splitting\n",
        "    frac_idx = idx[:frac_length]\n",
        "    frac_set = [dataset[i] for i in frac_idx]\n",
        "    return(frac_set)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "a23652ce-9132-443d-b176-72d055709d81",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "a23652ce-9132-443d-b176-72d055709d81",
        "executionStartTime": 1632950581031,
        "executionStopTime": 1632950581381
      },
      "source": [
        "def load_test_data(dataset):\n",
        "       \n",
        "    global EMBEDDING_SIZE\n",
        "    global api2indx\n",
        "    \n",
        "    X=[]\n",
        "    X_alt = []\n",
        "    y=[]\n",
        "    dict_indx = len(api2indx)\n",
        "    for data_list in dataset:\n",
        "        if data_list[-1] == -1:\n",
        "            continue\n",
        "        final_output = data_list[-1][1]\n",
        "        prev_out = torch.Tensor()\n",
        "        api_seq_x = []\n",
        "        api_seq_x_alt = []\n",
        "        api_seq_y = []\n",
        "        for data in data_list:        \n",
        "            if data == -1:\n",
        "                continue    \n",
        "            api = data[0]\n",
        "            if api2indx.get(api, -1) == -1: \n",
        "                api2indx[api] = dict_indx\n",
        "                dict_indx += 1\n",
        "                 \n",
        "            api_indx = api2indx[api]\n",
        "            input_list = data[2] #.get_input()\n",
        "            output_tensor = final_output #data.get_output()\n",
        "\n",
        "            it_pad = []\n",
        "            it_pad_alt = []\n",
        "            for input_tensor in input_list:\n",
        "                if input_tensor.shape == prev_out.shape and torch.all(input_tensor.eq(prev_out)).item():\n",
        "                    #same with previous output\n",
        "                    t = torch.zeros(EMBEDDING_SIZE + SHAPE_EMBEDDING_SIZE + 1 + 2)\n",
        "                    t[-1] = -1\n",
        "                    it_pad.append(t)\n",
        "                else:         \n",
        "                    #flatten the input tensor\n",
        "                    it_pad.append(tensor_flatten_pad(input_tensor,isNoise=True))\n",
        "\n",
        "                it_pad_alt.append(tensor_flatten_pad(input_tensor,isNoise=True))\n",
        "                \n",
        "            \n",
        "            #adding addidional tensors with zero embeddings for < 2 tensors\n",
        "            for i in range(len(it_pad),3):\n",
        "                t = torch.zeros(EMBEDDING_SIZE + SHAPE_EMBEDDING_SIZE + 1 + 2)\n",
        "                t[-1] = -1\n",
        "                it_pad.append(t)\n",
        "                it_pad_alt.append(t)\n",
        "            \n",
        "            ot_pad = tensor_flatten_pad(output_tensor, isNoise=True)\n",
        "            \n",
        "            x = T.flatten(T.stack((it_pad[0],it_pad[1], it_pad[2], ot_pad)))      \n",
        "            api_seq_x.append(x) \n",
        "            api_seq_y.append(api_indx)\n",
        "\n",
        "            x_alt = T.flatten(T.stack((it_pad_alt[0],it_pad_alt[1], it_pad_alt[2], ot_pad)))      \n",
        "            api_seq_x_alt.append(x_alt) \n",
        "\n",
        "\n",
        "            prev_out = data[1]\n",
        "\n",
        "        X.append(api_seq_x)\n",
        "        X_alt.append(api_seq_x_alt)\n",
        "        y.append(api_seq_y)\n",
        "    print(len(X), len(y))\n",
        "    return(X,X_alt, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "ac9bac65-cb69-49f4-a5d4-7aa9074c5daf",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "ac9bac65-cb69-49f4-a5d4-7aa9074c5daf",
        "executionStartTime": 1632950581962,
        "executionStopTime": 1632950582335
      },
      "source": [
        "api2indx = torch.load(f.Path + '/api2indx.pt')\n",
        "api2indx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "5852dc86-d057-4ee9-b190-6935bb9d93f5",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "5852dc86-d057-4ee9-b190-6935bb9d93f5",
        "executionStartTime": 1632950582682,
        "executionStopTime": 1632950655011
      },
      "source": [
        "SAVE_FILE = f.fuzzing\n",
        "i = 9\n",
        "fuzz_file = f.Path + '/fuzzing_data/' + str(i*10000) + '_' + SAVE_FILE\n",
        "embed_file = f.Path + '/' + str(i*10000) + '_test_embedding.pt' #+ f.embedding\n",
        "print(embed_file)\n",
        "l = torch.load(fuzz_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "c7375ee1-37f3-400b-b116-57e188d87894",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "c7375ee1-37f3-400b-b116-57e188d87894",
        "executionStartTime": 1632950655082,
        "executionStopTime": 1632950655953
      },
      "source": [
        "test_set = sample_dataset(l,0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "09f7b914-bec1-4506-a35e-1ee3c17789b8",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "09f7b914-bec1-4506-a35e-1ee3c17789b8",
        "executionStartTime": 1632950655962,
        "executionStopTime": 1632950656196
      },
      "source": [
        "len(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "9bd0e661-0324-4d2b-b96b-7fa6675045fe",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "9bd0e661-0324-4d2b-b96b-7fa6675045fe",
        "executionStartTime": 1632950656278,
        "executionStopTime": 1632950656436
      },
      "source": [
        "class FinalEmbedding:\n",
        "    def __init__(self,x,y):\n",
        "        self.x_data = x\n",
        "        self.y_data = y\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        preds = self.x_data[idx]\n",
        "        trgts = self.y_data[idx] \n",
        "        sample = { \n",
        "        'predictors' : preds,\n",
        "        'targets' : trgts\n",
        "        }\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "320989b4-6dec-4be8-acd5-14e5af790543",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "320989b4-6dec-4be8-acd5-14e5af790543",
        "executionStartTime": 1632950656503,
        "executionStopTime": 1632950656682,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "def process_dataX(tensor_list):\n",
        "    io_seq = []\n",
        "    \n",
        "    n0 = tensor_list[0]\n",
        "\n",
        "    if(len(tensor_list) == 1):\n",
        "        n1 = torch.zeros(n0.shape)\n",
        "        n2 = torch.zeros(n0.shape)\n",
        "        \n",
        "    elif(len(tensor_list) == 2):\n",
        "        n1 = tensor_list[1]\n",
        "        n2 = torch.zeros(n0.shape)\n",
        "\n",
        "    elif(len(tensor_list) == 3):\n",
        "        n1 = tensor_list[1]\n",
        "        n2 = tensor_list[2]\n",
        "\n",
        "    new_list = torch.stack((n0, n1, n2))\n",
        "    io_seq.append(new_list)\n",
        "    return(torch.stack(io_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "2f306d47-5ec4-43aa-a91d-a7caaf6a318f",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "2f306d47-5ec4-43aa-a91d-a7caaf6a318f",
        "executionStartTime": 1632950656751,
        "executionStopTime": 1632950656932
      },
      "source": [
        "indx2api = {}\n",
        "EOS = '<eol>'\n",
        "\n",
        "def process_dataY(api_seq):\n",
        "    global indx2api\n",
        "    global api2indx\n",
        "\n",
        "    ''' Add <eol> to the dictionary '''\n",
        "    indx2api = {v: k for k, v in api2indx.items()}\n",
        "\n",
        "    if api2indx.get(EOS, -1) == -1:\n",
        "        max_key = max(indx2api.keys())\n",
        "        print(max_key)\n",
        "        indx2api[max_key+1] = EOS\n",
        "        api2indx[EOS] = max_key+1\n",
        "\n",
        "    eos = api2indx[EOS]\n",
        "    api_tensors = []\n",
        "    api0 = api_seq[0]\n",
        "\n",
        "    if len(api_seq) == 1:\n",
        "        api1 = eos\n",
        "        api2 = eos\n",
        "\n",
        "    elif len(api_seq) == 2:\n",
        "        api1 = api_seq[1]\n",
        "        api2 = eos\n",
        "\n",
        "    elif len(api_seq) == 3:\n",
        "        api1 = api_seq[1]\n",
        "        api2 = api_seq[2]\n",
        "\n",
        "    else:\n",
        "        print('!!! Not supposed to be here')\n",
        "\n",
        "    t = torch.tensor([api0, api1, api2])\n",
        "    api_tensors.append(t)\n",
        "    \n",
        "    return(torch.stack(api_tensors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "edecf407-90ff-438e-ab7e-f2690b68c058",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "edecf407-90ff-438e-ab7e-f2690b68c058",
        "executionStartTime": 1632950657232,
        "executionStopTime": 1632950657411,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=True)   \n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        \n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out1 = out.contiguous().view(-1, self.hidden_dim*2)\n",
        "        out1 = self.fc(out1)\n",
        "        \n",
        "        return out1, hidden, out\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers*2, batch_size, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "ab94bc5f-c6a2-421f-991e-0918f40936ec",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "ab94bc5f-c6a2-421f-991e-0918f40936ec",
        "executionStartTime": 1632950657481,
        "executionStopTime": 1632950657647
      },
      "source": [
        "class FFNet(T.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FFNet, self).__init__()\n",
        "\n",
        "    self.hid1 = T.nn.Linear(4*(f.EMBEDDING_SIZE+f.SHAPE_EMBEDDING_SIZE+1+2), 500)\n",
        "    self.hid2 = T.nn.Linear(500, 250)\n",
        "    self.hid3 = T.nn.Linear(250, 100)\n",
        "    self.oupt = T.nn.Linear(100, len(api2indx))\n",
        "\n",
        "    T.nn.init.xavier_uniform_(self.hid1.weight)\n",
        "    T.nn.init.zeros_(self.hid1.bias)\n",
        "    T.nn.init.xavier_uniform_(self.hid2.weight)\n",
        "    T.nn.init.zeros_(self.hid2.bias)\n",
        "    T.nn.init.xavier_uniform_(self.oupt.weight)\n",
        "    T.nn.init.zeros_(self.oupt.bias)\n",
        "\n",
        "    T.nn.Dropout(p=0.2)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    z1 = T.tanh(self.hid1(x))\n",
        "    z2 = T.tanh(self.hid2(z1))\n",
        "    z3 = T.tanh(self.hid3(z2))\n",
        "    z = self.oupt(z3)  # no softmax: CrossEntropyLoss() \n",
        "    return (z, z3, z2, z1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "7fa7a162-59ea-4a7e-b117-2b079133020b",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "7fa7a162-59ea-4a7e-b117-2b079133020b",
        "executionStartTime": 1632950657715,
        "executionStopTime": 1632950657884
      },
      "source": [
        "DEBUG = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "2338ab6e-12be-4196-93c9-b053c2d15fbd",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "2338ab6e-12be-4196-93c9-b053c2d15fbd",
        "executionStartTime": 1632950657952,
        "executionStopTime": 1632950658136,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "def beam_search(top3_list):\n",
        "\n",
        "    api_seq = []\n",
        "    api_seq1 = []\n",
        "    for i in top3_list[0]:\n",
        "        for j in top3_list[1]:\n",
        "            api_seq1.append(((i[0], j[0]), i[1]*j[1]))\n",
        "    \n",
        "    api_seq1.sort(key = lambda x: x[1], reverse=True) \n",
        "\n",
        "    for k in top3_list[2]:\n",
        "        for s1 in api_seq1:\n",
        "            api_seq.append(((s1[0][0], s1[0][1], k[0]), s1[1]*k[1]))\n",
        "\n",
        "    api_seq.sort(key = lambda x: x[1], reverse=True) \n",
        "    \n",
        "    return(api_seq)\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "ec9b1f1f-7969-4163-9e14-64009360de37",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "ec9b1f1f-7969-4163-9e14-64009360de37",
        "executionStartTime": 1632950658211,
        "executionStopTime": 1632950658383,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "def query_model(X, Y):\n",
        "\n",
        "    with T.no_grad():\n",
        "      start_time = time.time()\n",
        "      predicts, z3, z2, z1 = net(X)\n",
        "      temp_z3 = torch.unsqueeze(z3,0)\n",
        "      model_output, hidden, int_output = rnn_model(temp_z3)\n",
        "\n",
        "      target_list = list(Y.cpu().numpy())\n",
        "    \n",
        "    h = (int_output,target_list,temp_z3)\n",
        "\n",
        "    top_indx = []\n",
        "\n",
        "    for m in model_output:\n",
        "        prob = nn.functional.softmax(m, dim=0).data\n",
        "        # Taking the class with the highest probability score from the output\n",
        "        api_ind = torch.max(prob, dim=0)[1].item()\n",
        "        top_indx.append(api_ind)\n",
        "        if DEBUG:\n",
        "          print(indx2api[api_ind])\n",
        "      \n",
        "    ed = 0\n",
        "\n",
        "    return(h, top_indx)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "7a41c202-8c9c-4d17-8220-1def9dbac21f",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "7a41c202-8c9c-4d17-8220-1def9dbac21f",
        "executionStartTime": 1632950658452,
        "executionStopTime": 1632950658638,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "def api_edit_distance(seq1, seq2):\n",
        "\n",
        "    edit_distantce = 0\n",
        "\n",
        "    for i in range(len(seq1)):\n",
        "        if seq1[i] != seq2[i]:\n",
        "            edit_distantce += 1\n",
        "\n",
        "    return(edit_distantce)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "5d4117ac-8ca0-4c51-9dfd-6c2695e7ece0",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "5d4117ac-8ca0-4c51-9dfd-6c2695e7ece0",
        "executionStartTime": 1632950658707,
        "executionStopTime": 1632950658882,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "max_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "55848a9f-7e22-45fa-b74f-d0fac0129201",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "55848a9f-7e22-45fa-b74f-d0fac0129201",
        "executionStartTime": 1632950658951,
        "executionStopTime": 1632950659132
      },
      "source": [
        "net = torch.load(f.Path + '/' + str(max_epochs) + '_train_net_model.pt')\n",
        "rnn_model = torch.load(f.Path + '/' + str(max_epochs) + '_train_rnn_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "336ec822-1eb5-4a8c-bf9a-17ccdbb07cdb",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "336ec822-1eb5-4a8c-bf9a-17ccdbb07cdb",
        "executionStartTime": 1632950659204,
        "executionStopTime": 1632950671331
      },
      "source": [
        "x, x_alt, y = load_test_data(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "931fb132-d04f-44e8-a451-a322fe586da7",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "931fb132-d04f-44e8-a451-a322fe586da7",
        "executionStartTime": 1632950671398,
        "executionStopTime": 1632950671511
      },
      "source": [
        "def validation_fos(x, x_alt, y):\n",
        "\n",
        "    total_query = 0\n",
        "    ncorrect = 0\n",
        "\n",
        "    for i in range(0,len(x_alt)):\n",
        "        for j in range(0,len(y[i])):\n",
        "            test_set_x1 = process_dataX([x_alt[i][j]])\n",
        "            test_set_y1 = process_dataY([y[i][j]])\n",
        "            ds = FinalEmbedding(test_set_x1,test_set_y1)\n",
        "            X1 = ds[0]['predictors'].to(device)\n",
        "            Y1 = ds[0]['targets'].to(device)\n",
        "            model_output1, top_index1 = query_model(X1,Y1)\n",
        "            total_query += 1\n",
        "            if top_index1[0] == Y1[0]:\n",
        "                ncorrect += 1\n",
        "\n",
        "            \n",
        "    print (total_query, ncorrect, ncorrect/total_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "f4a52164-891d-476f-8258-5c47eaf82dd6",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "f4a52164-891d-476f-8258-5c47eaf82dd6",
        "executionStartTime": 1632950671582,
        "executionStopTime": 1632950692002
      },
      "source": [
        "validation_fos(x, x_alt, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "bb35888b-0598-4981-9021-ddf0ef6f4b01",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "bb35888b-0598-4981-9021-ddf0ef6f4b01",
        "executionStartTime": 1632950692061,
        "executionStopTime": 1632950706944
      },
      "source": [
        "import scipy\n",
        "from scipy.spatial import distance\n",
        "from scipy import spatial\n",
        "import scipy.stats\n",
        "\n",
        "final_hiddens = []\n",
        "\n",
        "#comparing h2 of generative model with h1 of first in seq model\n",
        "hidden_states0 = []\n",
        "hidden_states1 = []\n",
        "y_axis = []\n",
        "\n",
        "ncorrect = 0\n",
        "sim_index = [0,0,0]\n",
        "\n",
        "similar = []\n",
        "dissimilar = []\n",
        "\n",
        "for i in range(0,len(x)):\n",
        "    if len(x[i]) < 2:\n",
        "        continue\n",
        "    if y[i][0] == y[i][1]:\n",
        "        continue #removing the same APIs to reduce noise\n",
        "    test_set_x = process_dataX(x[i])\n",
        "    test_set_y = process_dataY(y[i])\n",
        "    ds = FinalEmbedding(test_set_x,test_set_y)\n",
        "    X = ds[0]['predictors'].to(device)\n",
        "    Y = ds[0]['targets'].to(device)\n",
        "    model_output0, top_index0 = query_model(X,Y)\n",
        "\n",
        "    if top_index0[1] != Y[1]:\n",
        "        continue\n",
        "    else:\n",
        "        ncorrect += 1\n",
        "    \n",
        "    test_set_x1 = process_dataX([x_alt[i][1]])\n",
        "    test_set_y1 = process_dataY([y[i][1]])\n",
        "    ds = FinalEmbedding(test_set_x1,test_set_y1)\n",
        "    X1 = ds[0]['predictors'].to(device)\n",
        "    Y1 = ds[0]['targets'].to(device)\n",
        "    model_output1, top_index1 = query_model(X1,Y1)\n",
        "    \n",
        "    if top_index1[0] != Y1[0]:\n",
        "        continue\n",
        "\n",
        "    hiddens0 = model_output0[0].cpu()#.tolist()\n",
        "    hiddens1 = model_output1[0].cpu()#.tolist()\n",
        "\n",
        "    hiddens0 = hiddens0[0]\n",
        "    hiddens1 = hiddens1[0]\n",
        "\n",
        "    hidden_states0.append(hiddens0[1])\n",
        "    hidden_states1.append(hiddens1[0])\n",
        "    y_axis.append(Y1[0].item())\n",
        "\n",
        "    final_hiddens.append((hiddens0[0], hiddens0[1], hiddens0[2], hiddens1[0], Y1[0].item()))\n",
        "\n",
        "    results = []\n",
        "    for k in [0,1,2]:\n",
        "        result = 1 - distance.cosine(hiddens0[k], hiddens1[0])\n",
        "        results.append(result)\n",
        "        if k == 1:\n",
        "            similar.append(result)\n",
        "        elif k==0:\n",
        "            dissimilar.append(result)\n",
        "\n",
        "    max_value = max(results)\n",
        "    max_index = results.index(max_value)\n",
        "    sim_index[max_index] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "d613ac9d-e0ae-4b59-b43b-86aee3610d7e",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "d613ac9d-e0ae-4b59-b43b-86aee3610d7e",
        "executionStartTime": 1632950706950,
        "executionStopTime": 1632950707059
      },
      "source": [
        "scipy.stats.ttest_rel(dissimilar,similar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "cbd427b9-f2e3-4997-8673-d60f52f61ba2",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "cbd427b9-f2e3-4997-8673-d60f52f61ba2",
        "executionStartTime": 1632950707087,
        "executionStopTime": 1632950707532
      },
      "source": [
        "torch.save(final_hiddens,f.Path + '/final_hiddens.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "ab481663-9220-4f3f-8b12-3b8f6e5e2a84",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "ab481663-9220-4f3f-8b12-3b8f6e5e2a84",
        "executionStartTime": 1632950707599,
        "executionStopTime": 1632950711346
      },
      "source": [
        "''' Report h3 and h3' '''\n",
        "\n",
        "final_hiddens = []\n",
        "\n",
        "#comparing h2 of generative model with h1 of first in seq model\n",
        "hidden_states0 = []\n",
        "hidden_states1 = []\n",
        "y_axis = []\n",
        "\n",
        "ncorrect = 0\n",
        "sim_index = [0,0,0]\n",
        "n = 2\n",
        "\n",
        "similar = []\n",
        "dissimilar0 = []\n",
        "dissimilar1 = []\n",
        "\n",
        "for i in range(0,len(x)):\n",
        "    if len(x[i]) < 3:\n",
        "        continue\n",
        "    if y[i][n-1] == y[i][n]:\n",
        "        continue #removing the same APIs to reduce noise\n",
        "    test_set_x = process_dataX(x[i])\n",
        "    test_set_y = process_dataY(y[i])\n",
        "    ds = FinalEmbedding(test_set_x,test_set_y)\n",
        "    X = ds[0]['predictors'].to(device)\n",
        "    Y = ds[0]['targets'].to(device)\n",
        "    model_output0, top_index0 = query_model(X,Y)\n",
        "\n",
        "    if top_index0[n] != Y[n]:\n",
        "        continue\n",
        "    else:\n",
        "        ncorrect += 1\n",
        "    \n",
        "    test_set_x1 = process_dataX([x_alt[i][n]])\n",
        "    test_set_y1 = process_dataY([y[i][n]])\n",
        "\n",
        "    ds = FinalEmbedding(test_set_x1,test_set_y1)\n",
        "    X1 = ds[0]['predictors'].to(device)\n",
        "    Y1 = ds[0]['targets'].to(device)\n",
        "    model_output1, top_index1 = query_model(X1,Y1)\n",
        "\n",
        "    if top_index1[0] != top_index0[2]:\n",
        "        continue\n",
        "\n",
        "    hiddens0 = model_output0[0].cpu()#.tolist()\n",
        "    hiddens1 = model_output1[0].cpu()#.tolist()\n",
        "\n",
        "    hiddens0 = hiddens0[0]\n",
        "    hiddens1 = hiddens1[0]\n",
        "\n",
        "    hidden_states0.append(hiddens0[n])\n",
        "    hidden_states1.append(hiddens1[0])\n",
        "    y_axis.append(Y1[0].item())\n",
        "\n",
        "    final_hiddens.append((hiddens0[0], hiddens0[1], hiddens0[2], hiddens1[0], Y1[0].item()))\n",
        "\n",
        "    results = []\n",
        "    for k in [0,1,2]:\n",
        "        result = 1 - distance.cosine(hiddens0[k], hiddens1[0])\n",
        "        results.append(result)\n",
        "        if k == n:\n",
        "            similar.append(result)\n",
        "        elif k==0:\n",
        "            dissimilar0.append(result)\n",
        "        elif k==1:\n",
        "            dissimilar1.append(result)\n",
        "\n",
        "    max_value = max(results)\n",
        "    max_index = results.index(max_value)\n",
        "\n",
        "    sim_index[max_index] += 1\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "fdbc2023-6bce-411f-89ce-36f7b7538b35",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "fdbc2023-6bce-411f-89ce-36f7b7538b35",
        "executionStartTime": 1632950711353,
        "executionStopTime": 1632950711497
      },
      "source": [
        "np.median(dissimilar0),  np.median(dissimilar1), np.median(similar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "970e249d-12de-48b6-b22c-c0a3b32d2321",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "970e249d-12de-48b6-b22c-c0a3b32d2321",
        "executionStartTime": 1632950711506,
        "executionStopTime": 1632950711682
      },
      "source": [
        "def pad_or_truncate(some_list, target_len):\n",
        "    return some_list[:target_len] + [0]*(target_len - len(some_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "5bae6af5-6d52-4cdc-a7e6-788a0035cf1d",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "5bae6af5-6d52-4cdc-a7e6-788a0035cf1d",
        "executionStartTime": 1632950711762,
        "executionStopTime": 1632950750582,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "similar = []\n",
        "dissimilar = []\n",
        "\n",
        "for i in range(0,len(hidden_states0)):\n",
        "    for j in range(0,len(hidden_states1)):\n",
        "        result = 1 - spatial.distance.cosine(hidden_states0[i], hidden_states1[j])\n",
        "        # print(i, j, result)\n",
        "        if i == j:\n",
        "            similar.append(result)\n",
        "        else:\n",
        "            dissimilar.append(result)\n",
        "        #results[i][j] = result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "b8069da1-9079-4e53-943d-1e39718c88bd",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "b8069da1-9079-4e53-943d-1e39718c88bd",
        "executionStartTime": 1632950750632,
        "executionStopTime": 1632950750854
      },
      "source": [
        "t = scipy.stats.ttest_ind(similar, dissimilar)\n",
        "t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "ebd710b1-b4d5-4411-9d31-a7573acfc6ed",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "ebd710b1-b4d5-4411-9d31-a7573acfc6ed",
        "executionStartTime": 1632950750862,
        "executionStopTime": 1632950750933
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
